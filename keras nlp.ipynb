{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='train-v1.1.json'\n",
    "with open(datapath) as f1:\n",
    "    dictionary=json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user3\\\\Desktop\\\\divya ds\\\\divya\\\\ds projects\\\\DS projects'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-level-keys: ['data', 'version']\n",
      "data keys: ['title', 'paragraphs']\n",
      "paragraphs keys: ['context', 'qas']\n",
      "qas keys: ['answers', 'question', 'id']\n",
      "answers keys: ['answer_start', 'text']\n"
     ]
    }
   ],
   "source": [
    "#explore json files\n",
    "print('top-level-keys: {}'.format(list(dictionary.keys())))\n",
    "print('data keys: {}'.format(list(dictionary['data'][0].keys())))\n",
    "print('paragraphs keys: {}'.format(list(dictionary['data'][0]['paragraphs'][0].keys())))\n",
    "print('qas keys: {}'.format(list(dictionary['data'][0]['paragraphs'][0]['qas'][0].keys())))\n",
    "print('answers keys: {}'.format(list(dictionary['data'][0]['paragraphs'][0]['qas'][0]['answers'][0].keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary['data']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['University_of_Notre_Dame', 'Beyoncé', 'Montana', 'Genocide', 'Antibiotics', 'Frédéric_Chopin', 'Sino-Tibetan_relations_during_the_Ming_dynasty', 'IPod', 'The_Legend_of_Zelda:_Twilight_Princess', 'Spectre_(2015_film)', '2008_Sichuan_earthquake', 'New_York_City', 'To_Kill_a_Mockingbird', 'Solar_energy', 'Tajikistan', 'Anthropology', 'Portugal', 'Kanye_West', 'Buddhism', 'American_Idol', 'Dog', '2008_Summer_Olympics_torch_relay', 'Alfred_North_Whitehead', 'Financial_crisis_of_2007%E2%80%9308', 'Saint_Barth%C3%A9lemy', 'Genome', 'Comprehensive_school', 'Republic_of_the_Congo', 'Prime_minister', 'Institute_of_technology', 'Wayback_Machine', 'Dutch_Republic', 'Symbiosis', 'Canadian_Armed_Forces', 'Cardinal_(Catholicism)', 'Iranian_languages', 'Lighting', 'Separation_of_powers_under_the_United_States_Constitution', 'Architecture', 'Human_Development_Index', 'Southern_Europe', 'BBC_Television', 'Arnold_Schwarzenegger', 'Plymouth', 'Heresy', 'Warsaw_Pact', 'Materialism', 'Space_Race', 'Pub', 'Christian', 'Sony_Music_Entertainment', 'Oklahoma_City', 'Hunter-gatherer', 'United_Nations_Population_Fund', 'Russian_Soviet_Federative_Socialist_Republic', 'Universal_Studios', 'Alexander_Graham_Bell', 'Internet_service_provider', 'Comics', 'Saint_Helena', 'Aspirated_consonant', 'Hydrogen', 'Web_browser', 'Boston', 'BeiDou_Navigation_Satellite_System', 'Canon_law', 'Communications_in_Somalia', 'Catalan_language', 'Estonian_language', 'Paper', 'Arena_Football_League', 'Adult_contemporary_music', 'Matter', 'Westminster_Abbey', 'Nanjing', 'Bern', 'Daylight_saving_time', 'Royal_Institute_of_British_Architects', 'National_Archives_and_Records_Administration', 'Tristan_da_Cunha', 'University_of_Kansas', 'Political_corruption', 'Dialect', 'Classical_music', 'Slavs', 'Southampton', 'Treaty', 'Josip_Broz_Tito', 'Marshall_Islands', 'Szlachta', 'Virgil', 'Alps', 'Gene', 'Guinea-Bissau', 'List_of_numbered_streets_in_Manhattan', 'Brain', 'Near_East', 'Zhejiang', 'Ministry_of_Defence_(United_Kingdom)', 'High-definition_television', 'Wood', 'Somalis', 'Middle_Ages', 'Phonology', 'Computer', 'Black_people', 'The_Times', 'New_Delhi', 'Imamah_(Shia_doctrine)', 'Bird_migration', 'Atlantic_City,_New_Jersey', 'Immunology', 'MP3', 'House_music', 'Letter_case', 'Chihuahua_(state)', 'Pitch_(music)', 'England_national_football_team', 'Houston', 'Copper', 'Identity_(social_science)', 'Himachal_Pradesh', 'Communication', 'Grape', 'Computer_security', 'Orthodox_Judaism', 'Animal', 'Beer', 'Race_and_ethnicity_in_the_United_States_Census', 'United_States_dollar', 'Imperial_College_London', 'Gymnastics', 'Hanover', 'Emotion', 'FC_Barcelona', 'Everton_F.C.', 'Old_English', 'Aircraft_carrier', 'Federal_Aviation_Administration', 'Lancashire', 'Mesozoic', 'Videoconferencing', 'Gregorian_calendar', 'Xbox_360', 'Military_history_of_the_United_States', 'Hard_rock', 'Great_Plains', 'Infrared', 'Biodiversity', 'ASCII', 'Digestion', 'Federal_Bureau_of_Investigation', 'Adolescence', 'Antarctica', 'Mary_(mother_of_Jesus)', 'Melbourne', 'John,_King_of_England', 'Macintosh', 'Anti-aircraft_warfare', 'Sanskrit', 'Valencia', 'General_Electric', 'United_States_Army', 'Franco-Prussian_War', 'Eritrea', 'Uranium', 'Order_of_the_British_Empire', 'Age_of_Enlightenment', 'Circadian_rhythm', 'Elizabeth_II', 'Sexual_orientation', 'Dell', 'Capital_punishment_in_the_United_States', 'Nintendo_Entertainment_System', 'Ashkenazi_Jews', 'Athanasius_of_Alexandria', 'Seattle', 'Memory', 'Multiracial_American', 'Pharmaceutical_industry', 'Umayyad_Caliphate', 'Asphalt', 'Queen_Victoria', 'Freemasonry', 'Israel', 'Hellenistic_period', 'Napoleon', 'Bill_%26_Melinda_Gates_Foundation', 'Northwestern_University', 'Hokkien', 'Montevideo', 'Poultry', 'Arsenal_F.C.', 'Dutch_language', 'Buckingham_Palace', 'Incandescent_light_bulb', 'Clothing', 'Chicago_Cubs', 'States_of_Germany', 'Korean_War', 'Royal_Dutch_Shell', 'Copyright_infringement', 'Greece', 'Mammal', 'East_India_Company', 'Southeast_Asia', 'Professional_wrestling', 'Film_speed', 'Mexico_City', 'Germans', 'New_Haven,_Connecticut', 'Brigham_Young_University', 'Myocardial_infarction', 'Department_store', 'Intellectual_property', 'Florida', 'Queen_(band)', 'Presbyterianism', 'Thuringia', 'Predation', 'Marvel_Comics', 'British_Empire', 'Botany', 'Madonna_(entertainer)', 'London', 'Law_of_the_United_States', 'Myanmar', 'Jews', 'Cotton', 'Data_compression', 'The_Sun_(United_Kingdom)', 'Carnival', 'Pesticide', 'Somerset', 'Yale_University', 'Late_Middle_Ages', 'Ann_Arbor,_Michigan', 'Gothic_architecture', 'Cubism', 'Political_philosophy', 'Alloy', 'Norfolk_Island', 'Edmund_Burke', 'Samoa', 'Pope_Paul_VI', 'George_VI', 'Electric_motor', 'Switzerland', 'Mali', 'Nonprofit_organization', 'Raleigh,_North_Carolina', 'Nutrition', 'Crimean_War', 'Literature', 'Avicenna', 'Chinese_characters', 'Bermuda', 'Nigeria', 'Utrecht', 'John_von_Neumann', 'Molotov%E2%80%93Ribbentrop_Pact', 'Capacitor', 'History_of_science', 'Czech_language', 'Digimon', 'Glacier', 'Planck_constant', 'Comcast', 'Tuberculosis', 'Affirmative_action_in_the_United_States', 'FA_Cup', 'Alsace', 'Baptists', 'Child_labour', 'North_Carolina', 'Heian_period', 'On_the_Origin_of_Species', 'Dissolution_of_the_Soviet_Union', 'Crucifixion_of_Jesus', 'Miami', 'Supreme_court', 'Textual_criticism', 'Gramophone_record', 'Turner_Classic_Movies', 'Hindu_philosophy', 'Political_party', 'A_cappella', 'Dominican_Order', 'Eton_College', 'Cork_(city)', 'Federalism', 'Galicia_(Spain)', 'Green', 'USB', 'Sichuan', 'Unicode', 'Detroit', 'Culture', 'Sahara', 'Rule_of_law', 'Tibet', 'Exhibition_game', 'Strasbourg', 'Oklahoma', 'History_of_India', 'Gamal_Abdel_Nasser', 'Pope_John_XXIII', 'Time', 'European_Central_Bank', 'St._John%27s,_Newfoundland_and_Labrador', 'PlayStation_3', 'Royal_assent', 'Group_(mathematics)', 'Central_African_Republic', 'Asthma', 'LaserDisc', 'Annelid', 'God', 'War_on_Terror', 'Labour_Party_(UK)', 'Estonia', 'Serbo-Croatian', 'Alaska', 'Karl_Popper', 'Mandolin', 'Insect', 'Race_(human_categorization)', 'Paris', 'Apollo', 'United_States_presidential_election,_2004', 'IBM', 'Liberal_Party_of_Australia', 'Samurai', 'Software_testing', 'Glass', 'Renewable_energy_commercialization', 'Palermo', 'Zinc', 'Neoclassical_architecture', 'CBC_Television', 'Appalachian_Mountains', 'Energy', 'East_Prussia', 'Ottoman_Empire', 'Philosophy_of_space_and_time', 'Neolithic', 'Friedrich_Hayek', 'Diarrhea', 'Madrasa', 'Philadelphia', 'John_Kerry', 'Rajasthan', 'Guam', 'Empiricism', 'Idealism', 'Education', 'Tennessee', 'Post-punk', 'Canadian_football', 'Seven_Years%27_War', 'Richard_Feynman', 'Muammar_Gaddafi', 'Cyprus', 'Steven_Spielberg', 'Elevator', 'Neptune', 'Railway_electrification_system', 'Spanish_language_in_the_United_States', 'Charleston,_South_Carolina', 'Red', 'The_Blitz', 'Endangered_Species_Act', 'Vacuum', 'Han_dynasty', 'Greeks', 'Quran', 'Great_power', 'Geography_of_the_United_States', 'Compact_disc', 'Transistor', 'Modern_history', '51st_state', 'Antenna_(radio)', 'Flowering_plant', 'Hyderabad', 'Santa_Monica,_California', 'Washington_University_in_St._Louis', 'Central_Intelligence_Agency', 'Pain', 'Database', 'Tucson,_Arizona', 'Armenia', 'Bacteria', 'Printed_circuit_board', 'Premier_League', 'Roman_Republic', 'Pacific_War', 'Richmond,_Virginia', 'San_Diego', 'Muslim_world', 'Iran', 'British_Isles', 'Association_football', 'Georgian_architecture', 'Liberia', 'Windows_8', 'Swaziland', 'Translation', 'Airport', 'Kievan_Rus%27', 'Super_Nintendo_Entertainment_System', 'Sumer', 'Tuvalu', 'Immaculate_Conception', 'Namibia', 'Russian_language', 'United_States_Air_Force', 'Light-emitting_diode', 'Bird', 'Qing_dynasty', 'Indigenous_peoples_of_the_Americas', 'Egypt', 'Mosaic', 'University', 'Religion_in_ancient_Rome', 'YouTube', 'Separation_of_church_and_state_in_the_United_States', 'Protestantism', 'Bras%C3%ADlia', 'Economy_of_Greece', 'Party_leaders_of_the_United_States_House_of_Representatives', 'Armenians', 'Jehovah%27s_Witnesses', 'Dwight_D._Eisenhower', 'The_Bronx', 'Humanism', 'Geological_history_of_Earth', 'Police', 'Punjab,_Pakistan', 'Infection', 'Hunting', 'Kathmandu']\n"
     ]
    }
   ],
   "source": [
    "# Print Corpora Titles\n",
    "print(list(json_normalize(dictionary,'data')['title']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_squad_to_tidy_df(df, corpus):\n",
    "    data = [c for c in df['data'] if c['title']==corpus][0]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    data_paragraphs = data['paragraphs']\n",
    "    for article_dict in data_paragraphs:\n",
    "        row = []\n",
    "        for answers_dict in article_dict['qas']:\n",
    "            for answer in answers_dict['answers']:\n",
    "                row.append((article_dict['context'], \n",
    "                            answers_dict['question'], \n",
    "                            answers_dict['id'],\n",
    "                            answer['answer_start'],\n",
    "                            answer['text']\n",
    "                           ))\n",
    "        df = pd.concat([df, pd.DataFrame.from_records(row, columns=['context', 'question', 'id', 'answer_start', 'Answer'])], axis=0, ignore_index=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cambridge English Dictionary states that cultu...</td>\n",
       "      <td>How does the Cambridge English Dictionary def...</td>\n",
       "      <td>5727b6064b864d1900163b0a</td>\n",
       "      <td>54</td>\n",
       "      <td>the way of life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cambridge English Dictionary states that cultu...</td>\n",
       "      <td>What is the name of this theory that culture i...</td>\n",
       "      <td>5727b6064b864d1900163b0b</td>\n",
       "      <td>170</td>\n",
       "      <td>Terror Management Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cambridge English Dictionary states that cultu...</td>\n",
       "      <td>What did Homo Sapiens acquire that allowed the...</td>\n",
       "      <td>5727b6064b864d1900163b0c</td>\n",
       "      <td>518</td>\n",
       "      <td>larger brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a defining aspect of what it means to be hu...</td>\n",
       "      <td>Around when were humans able to understand to ...</td>\n",
       "      <td>5727ba4b4b864d1900163b9c</td>\n",
       "      <td>430</td>\n",
       "      <td>50,000 years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As a defining aspect of what it means to be hu...</td>\n",
       "      <td>Name some cultural universals that exist with ...</td>\n",
       "      <td>5727ba4b4b864d1900163b9d</td>\n",
       "      <td>921</td>\n",
       "      <td>kinship, gender and marriage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Cambridge English Dictionary states that cultu...   \n",
       "1  Cambridge English Dictionary states that cultu...   \n",
       "2  Cambridge English Dictionary states that cultu...   \n",
       "3  As a defining aspect of what it means to be hu...   \n",
       "4  As a defining aspect of what it means to be hu...   \n",
       "\n",
       "                                            question  \\\n",
       "0   How does the Cambridge English Dictionary def...   \n",
       "1  What is the name of this theory that culture i...   \n",
       "2  What did Homo Sapiens acquire that allowed the...   \n",
       "3  Around when were humans able to understand to ...   \n",
       "4  Name some cultural universals that exist with ...   \n",
       "\n",
       "                         id  answer_start                        Answer  \n",
       "0  5727b6064b864d1900163b0a            54               the way of life  \n",
       "1  5727b6064b864d1900163b0b           170      Terror Management Theory  \n",
       "2  5727b6064b864d1900163b0c           518                  larger brain  \n",
       "3  5727ba4b4b864d1900163b9c           430              50,000 years ago  \n",
       "4  5727ba4b4b864d1900163b9d           921  kinship, gender and marriage  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'Culture' \n",
    "# corpus = 'Culture' # only in train dataset\n",
    "\n",
    "df1 = convert_squad_to_tidy_df(dictionary, corpus)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culture\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lst=[df1['context'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cambridge English Dictionary states that culture is, \"the way of life, especially the general customs and beliefs, of a particular group of people at a particular time.\" Terror Management Theory posits that culture is a series of activities and worldviews that provide humans with the illusion of being individuals of value in a world meaning—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that Homo Sapiens became aware of when they acquired a larger brain.']\n"
     ]
    }
   ],
   "source": [
    "print(context_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "passage=[]\n",
    "for i in context_lst:\n",
    "    passage.append(re.sub(r'[''\"\",.]+','',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cambridge English Dictionary states that culture is the way of life especially the general customs and beliefs of a particular group of people at a particular time Terror Management Theory posits that culture is a series of activities and worldviews that provide humans with the illusion of being individuals of value in a world meaning—raising themselves above the merely physical aspects of existence in order to deny the animal insignificance and death that Homo Sapiens became aware of when they acquired a larger brain']\n"
     ]
    }
   ],
   "source": [
    "print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How does the Cambridge English Dictionary define \"Culture\" in short?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the way of life'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Cambridge', 'English', 'Dictionary', 'states', 'that', 'culture', 'is', 'the', 'way', 'of', 'life', 'especially', 'the', 'general', 'customs', 'and', 'beliefs', 'of', 'a', 'particular', 'group', 'of', 'people', 'at', 'a', 'particular', 'time', 'Terror', 'Management', 'Theory', 'posits', 'that', 'culture', 'is', 'a', 'series', 'of', 'activities', 'and', 'worldviews', 'that', 'provide', 'humans', 'with', 'the', 'illusion', 'of', 'being', 'individuals', 'of', 'value', 'in', 'a', 'world', 'meaning—raising', 'themselves', 'above', 'the', 'merely', 'physical', 'aspects', 'of', 'existence', 'in', 'order', 'to', 'deny', 'the', 'animal', 'insignificance', 'and', 'death', 'that', 'Homo', 'Sapiens', 'became', 'aware', 'of', 'when', 'they', 'acquired', 'a', 'larger', 'brain']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "passage_tokens=[]\n",
    "for i in passage:\n",
    "    passage_tokens.append(word_tokenize(i))\n",
    "print(passage_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'against', 'about', 'not', 'doesn', \"should've\", 'few', 'couldn', 'didn', \"you're\", 'him', 'our', 'those', 'from', \"you'd\", 'these', 'some', 'haven', 't', 'through', 'me', 'doing', 'will', 'same', 'mustn', 'an', 'if', 'yourselves', 'she', \"it's\", 'did', 'no', 'you', 'had', 're', 'their', 'at', 'ours', 'do', 'and', 'they', 'wasn', 'yourself', 'then', \"didn't\", 'herself', 'been', 'hasn', \"hasn't\", 'hers', 've', 'does', 'on', 'further', 'nor', 'are', \"she's\", 'ain', 'where', 'itself', 'your', 'needn', \"that'll\", 'for', 'because', 'when', 'below', 'o', \"won't\", 'being', 'this', \"you'll\", 'after', 'hadn', \"weren't\", 'here', 'her', 'won', \"shan't\", \"mustn't\", 'who', 'just', 'were', 'very', 'between', 'what', 'until', 'up', 'own', \"aren't\", 'above', 'has', 'myself', \"shouldn't\", 'the', 'there', 'again', 'more', 'aren', \"hadn't\", 'it', 'have', \"mightn't\", 'while', 'themselves', 'all', 'with', 'in', 'theirs', \"isn't\", 'each', 'is', 'or', 'of', 'isn', 'mightn', 'himself', 'i', 'as', 'my', 'than', 'only', 'both', 'was', 'under', 'll', 'how', \"haven't\", 'we', 'them', \"doesn't\", 'into', \"wasn't\", 'during', 'can', 'so', 'before', 'am', 'yours', 'such', 'whom', 'his', 's', 'other', 'which', 'but', 'ourselves', 'shan', 'over', 'down', 'ma', \"wouldn't\", 'd', 'off', 'm', 'be', 'most', \"you've\", 'why', 'now', 'weren', 'he', 'once', \"couldn't\", 'shouldn', 'wouldn', 'y', 'too', 'should', 'to', \"don't\", 'having', 'its', 'any', 'that', 'don', 'out', 'a', \"needn't\", 'by'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "var=set(stopwords.words('english'))\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "for i in passage_tokens:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' How does the Cambridge English Dictionary define \"Culture\" in short?']\n"
     ]
    }
   ],
   "source": [
    "Question=[df1['question'][0]]\n",
    "print(Question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' How does the Cambridge English Dictionary define Culture in short']\n"
     ]
    }
   ],
   "source": [
    "ques=[]\n",
    "for i in Question:\n",
    "    ques.append(re.sub(r'[\"\"?]+','',i))\n",
    "print(ques)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['How', 'does', 'the', 'Cambridge', 'English', 'Dictionary', 'define', 'Culture', 'in', 'short']]\n"
     ]
    }
   ],
   "source": [
    "Ques_tokens=[]\n",
    "for i in ques:\n",
    "    Ques_tokens.append(word_tokenize(i))\n",
    "print(Ques_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in Ques_tokens:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the way of life']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=[df1['Answer'][0]]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'way', 'of', 'life']]\n"
     ]
    }
   ],
   "source": [
    "answer=[]\n",
    "for i in ans:\n",
    "    answer.append(word_tokenize(i))\n",
    "print(answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "embed=Word2Vec(passage_tokens,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed1=Word2Vec(Ques_tokens,min_count=1,size=100,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cambridge': <gensim.models.keyedvectors.Vocab object at 0x0000004732C2B6A0>, 'English': <gensim.models.keyedvectors.Vocab object at 0x0000004723CA4C88>, 'Dictionary': <gensim.models.keyedvectors.Vocab object at 0x0000004723CB29B0>, 'states': <gensim.models.keyedvectors.Vocab object at 0x000000473323DBE0>, 'that': <gensim.models.keyedvectors.Vocab object at 0x0000004733363EB8>, 'culture': <gensim.models.keyedvectors.Vocab object at 0x00000047334746D8>, 'is': <gensim.models.keyedvectors.Vocab object at 0x0000004733474710>, 'the': <gensim.models.keyedvectors.Vocab object at 0x0000004733474748>, 'way': <gensim.models.keyedvectors.Vocab object at 0x0000004733474780>, 'of': <gensim.models.keyedvectors.Vocab object at 0x00000047334747B8>, 'life': <gensim.models.keyedvectors.Vocab object at 0x00000047334747F0>, 'especially': <gensim.models.keyedvectors.Vocab object at 0x0000004733474828>, 'general': <gensim.models.keyedvectors.Vocab object at 0x0000004733474860>, 'customs': <gensim.models.keyedvectors.Vocab object at 0x0000004733474898>, 'and': <gensim.models.keyedvectors.Vocab object at 0x00000047334748D0>, 'beliefs': <gensim.models.keyedvectors.Vocab object at 0x0000004733474908>, 'a': <gensim.models.keyedvectors.Vocab object at 0x0000004733474940>, 'particular': <gensim.models.keyedvectors.Vocab object at 0x0000004733474978>, 'group': <gensim.models.keyedvectors.Vocab object at 0x00000047334749B0>, 'people': <gensim.models.keyedvectors.Vocab object at 0x00000047334749E8>, 'at': <gensim.models.keyedvectors.Vocab object at 0x0000004733474A20>, 'time': <gensim.models.keyedvectors.Vocab object at 0x0000004733474A58>, 'Terror': <gensim.models.keyedvectors.Vocab object at 0x0000004733474A90>, 'Management': <gensim.models.keyedvectors.Vocab object at 0x0000004733474AC8>, 'Theory': <gensim.models.keyedvectors.Vocab object at 0x0000004733474B00>, 'posits': <gensim.models.keyedvectors.Vocab object at 0x0000004733474B38>, 'series': <gensim.models.keyedvectors.Vocab object at 0x0000004733474B70>, 'activities': <gensim.models.keyedvectors.Vocab object at 0x0000004733474BA8>, 'worldviews': <gensim.models.keyedvectors.Vocab object at 0x0000004733474BE0>, 'provide': <gensim.models.keyedvectors.Vocab object at 0x0000004733474C18>, 'humans': <gensim.models.keyedvectors.Vocab object at 0x0000004733474C50>, 'with': <gensim.models.keyedvectors.Vocab object at 0x0000004733474C88>, 'illusion': <gensim.models.keyedvectors.Vocab object at 0x0000004733474CC0>, 'being': <gensim.models.keyedvectors.Vocab object at 0x0000004733474CF8>, 'individuals': <gensim.models.keyedvectors.Vocab object at 0x0000004733474D30>, 'value': <gensim.models.keyedvectors.Vocab object at 0x0000004733474D68>, 'in': <gensim.models.keyedvectors.Vocab object at 0x0000004733474DA0>, 'world': <gensim.models.keyedvectors.Vocab object at 0x0000004733474DD8>, 'meaning—raising': <gensim.models.keyedvectors.Vocab object at 0x0000004733474E10>, 'themselves': <gensim.models.keyedvectors.Vocab object at 0x0000004733474E48>, 'above': <gensim.models.keyedvectors.Vocab object at 0x0000004733474E80>, 'merely': <gensim.models.keyedvectors.Vocab object at 0x0000004733474EB8>, 'physical': <gensim.models.keyedvectors.Vocab object at 0x0000004733474EF0>, 'aspects': <gensim.models.keyedvectors.Vocab object at 0x0000004733474F28>, 'existence': <gensim.models.keyedvectors.Vocab object at 0x0000004733474F60>, 'order': <gensim.models.keyedvectors.Vocab object at 0x0000004733474F98>, 'to': <gensim.models.keyedvectors.Vocab object at 0x0000004733474FD0>, 'deny': <gensim.models.keyedvectors.Vocab object at 0x000000473347B048>, 'animal': <gensim.models.keyedvectors.Vocab object at 0x000000473347B080>, 'insignificance': <gensim.models.keyedvectors.Vocab object at 0x000000473347B0B8>, 'death': <gensim.models.keyedvectors.Vocab object at 0x000000473347B0F0>, 'Homo': <gensim.models.keyedvectors.Vocab object at 0x000000473347B128>, 'Sapiens': <gensim.models.keyedvectors.Vocab object at 0x000000473347B160>, 'became': <gensim.models.keyedvectors.Vocab object at 0x000000473347B198>, 'aware': <gensim.models.keyedvectors.Vocab object at 0x000000473347B1D0>, 'when': <gensim.models.keyedvectors.Vocab object at 0x000000473347B208>, 'they': <gensim.models.keyedvectors.Vocab object at 0x000000473347B240>, 'acquired': <gensim.models.keyedvectors.Vocab object at 0x000000473347B278>, 'larger': <gensim.models.keyedvectors.Vocab object at 0x000000473347B2B0>, 'brain': <gensim.models.keyedvectors.Vocab object at 0x000000473347B2E8>}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_C = embed.wv.vocab\n",
    "print(vocabulary_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.3131284e-03  3.9945976e-03 -7.7185978e-04 -8.9521601e-04\n",
      "  1.6791754e-03  2.2933946e-03  1.1908709e-03 -1.7499998e-03\n",
      "  4.4607143e-03 -1.1986205e-03 -2.0359772e-04 -1.5809394e-03\n",
      "  2.2498339e-03  1.0412659e-04 -1.3157674e-03 -1.7470740e-03\n",
      " -2.7552191e-03 -3.6838236e-03  3.1230685e-03 -4.2360136e-03\n",
      "  4.9897092e-03  1.1247137e-03  3.1228093e-03  4.5396872e-03\n",
      "  1.3105823e-03  2.6650357e-04  5.6896318e-04  2.7973906e-03\n",
      " -4.7291680e-03  4.3134196e-03  2.4100679e-03 -5.2302436e-04\n",
      "  4.6551991e-03 -2.3991931e-03  4.3359655e-03  3.9226194e-03\n",
      "  1.3693769e-03  1.8082054e-03  3.5176452e-04 -4.1667898e-03\n",
      " -3.3871941e-03 -5.1968358e-04 -2.9822839e-03  3.5567023e-03\n",
      "  3.4050852e-05  3.0971211e-03  1.5265443e-03  3.0038795e-03\n",
      "  4.6259020e-03 -4.5967991e-03 -1.4366913e-03  3.4419270e-03\n",
      "  2.7473946e-03  3.4949853e-04 -2.1339078e-03 -3.8713026e-03\n",
      "  1.3880533e-03 -3.2170929e-03  4.4957800e-03  2.8546604e-03\n",
      "  5.9699529e-04  8.9573377e-04 -4.4127116e-03  1.6245389e-03\n",
      "  3.6448182e-04  1.9219160e-04 -4.5789885e-03 -4.4491659e-03\n",
      " -2.5170380e-03  4.6369163e-03  2.4746242e-03  1.2582680e-03\n",
      "  3.7015041e-03 -2.4200773e-03 -2.5370277e-03  5.3313538e-04\n",
      "  2.9989840e-03  9.2192914e-04  4.8320447e-03  2.2383519e-03\n",
      "  4.1262130e-03  3.2138586e-04 -4.0472744e-04 -5.9676298e-04\n",
      "  2.1406297e-04  1.1164830e-03  4.0023224e-03  3.9069721e-04\n",
      "  4.8980140e-03  2.6255818e-03  3.5777883e-04  2.8195397e-03\n",
      "  3.3825061e-03  2.1455162e-03  4.6526347e-03 -3.7004184e-03\n",
      "  1.1940550e-03 -1.6447952e-03  2.1663243e-03 -1.9158943e-03]\n"
     ]
    }
   ],
   "source": [
    "vocabulary=embed.wv['Cambridge']\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': <gensim.models.keyedvectors.Vocab object at 0x000000473347B320>, 'does': <gensim.models.keyedvectors.Vocab object at 0x000000473347B6D8>, 'the': <gensim.models.keyedvectors.Vocab object at 0x000000473347B710>, 'Cambridge': <gensim.models.keyedvectors.Vocab object at 0x000000473347B748>, 'English': <gensim.models.keyedvectors.Vocab object at 0x000000473347B4E0>, 'Dictionary': <gensim.models.keyedvectors.Vocab object at 0x000000473347B518>, 'define': <gensim.models.keyedvectors.Vocab object at 0x000000473347B550>, 'Culture': <gensim.models.keyedvectors.Vocab object at 0x000000473347B588>, 'in': <gensim.models.keyedvectors.Vocab object at 0x000000473347B5C0>, 'short': <gensim.models.keyedvectors.Vocab object at 0x000000473347B5F8>}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_Q=embed1.wv.vocab\n",
    "print(vocabulary_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.1315451e-03  2.5469942e-03  1.0303527e-03 -7.3504105e-04\n",
      "  4.3116757e-03  3.8294930e-03 -2.2439160e-03  1.0839941e-03\n",
      " -3.4197029e-03 -2.4831428e-03 -3.3323569e-03  7.9966016e-04\n",
      "  1.5945311e-03 -1.6197244e-03  4.2471336e-03  3.9473749e-03\n",
      "  1.9133560e-03 -1.9977226e-04  3.3418899e-03  6.2997726e-04\n",
      " -4.9204538e-03  3.4122027e-03 -4.6739569e-03 -1.8185474e-03\n",
      "  4.7674063e-03  2.9801286e-03  2.9144732e-03  1.9205642e-03\n",
      " -2.2143542e-03  1.0666020e-03 -2.7987920e-03 -1.0303856e-03\n",
      "  4.1532181e-03 -1.7315440e-03  2.3522698e-03 -4.9523627e-03\n",
      " -7.7576499e-04  6.3131750e-04 -3.0373202e-03 -4.6365121e-03\n",
      "  4.9979114e-03 -1.9932820e-03  9.8939287e-04 -2.5404578e-03\n",
      "  4.7337124e-03 -4.2958506e-03  4.6861549e-03  2.6324196e-03\n",
      " -2.5248732e-03 -3.3936151e-03 -1.2363609e-03  1.2072254e-03\n",
      " -1.9202472e-04  1.4887335e-03  2.6822581e-03 -4.5562987e-03\n",
      " -2.3793811e-03  1.2770586e-03  3.3308058e-03  4.3676868e-03\n",
      "  4.0715579e-03  4.5067430e-03 -1.6086536e-03 -1.2966250e-03\n",
      "  4.9188668e-03 -4.2206375e-03 -4.8270836e-03 -2.3041267e-03\n",
      "  1.8421782e-03  8.4167498e-04 -2.4432435e-03  5.3668616e-04\n",
      "  3.6893389e-03  3.3766704e-03  4.8737098e-03 -3.0882084e-03\n",
      " -2.1544669e-03 -9.2798786e-05  3.5327456e-03 -2.3014541e-03\n",
      "  3.6426540e-03 -4.0388326e-03  1.6915343e-03  2.3860049e-03\n",
      " -3.1398826e-03  1.5648821e-03 -1.1720780e-03 -1.4743699e-03\n",
      " -2.3559385e-03  1.8687613e-03 -3.8185406e-03 -3.3299758e-03\n",
      " -4.9883546e-03 -1.8803484e-04  5.9926190e-04  9.1490382e-04\n",
      " -4.3202573e-03  1.8203588e-03  2.1999800e-03  2.7031475e-03]\n"
     ]
    }
   ],
   "source": [
    "vocabulary1=embed1.wv['does']\n",
    "print(vocabulary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in [vocabulary_C,vocabulary_Q]:\n",
    "    data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed3=Word2Vec(answer,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA20>, 'way': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA58>, 'of': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA90>, 'life': <gensim.models.keyedvectors.Vocab object at 0x000000473347BAC8>}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_A=embed3.wv.vocab\n",
    "print(vocabulary_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'the': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA20>, 'way': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA58>, 'of': <gensim.models.keyedvectors.Vocab object at 0x000000473347BA90>, 'life': <gensim.models.keyedvectors.Vocab object at 0x000000473347BAC8>}]\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "for i in [vocabulary_A]:\n",
    "    y.append(i)\n",
    "print(y)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.2740935e-03  3.8492619e-03  7.4336980e-04 -9.9029602e-04\n",
      "  8.0403006e-05  8.0445316e-04 -1.5031304e-03  2.5619154e-03\n",
      "  3.2559803e-03  4.9919863e-03 -3.0767913e-03  2.9857657e-03\n",
      " -2.9508562e-03  2.6959932e-04  4.4395597e-04  2.6866947e-03\n",
      " -2.7717638e-03 -4.1447314e-03  3.3507945e-03 -1.8660781e-03\n",
      " -1.2788426e-03 -5.5302144e-04 -1.7881498e-03 -1.9302040e-03\n",
      " -2.4963568e-03  1.6672513e-03  2.4012760e-03 -3.6082058e-03\n",
      "  3.8513062e-03  4.9019018e-03  2.3248782e-03  4.7842884e-03\n",
      "  3.3835508e-03 -3.8338362e-03 -4.8945989e-03  4.3900805e-03\n",
      " -4.7576084e-04 -2.0090593e-03 -4.9986970e-03  2.3446374e-03\n",
      "  1.4561601e-03  7.4700429e-04  4.3612621e-03  7.7691156e-04\n",
      " -1.5888023e-03 -3.1461562e-03 -3.8130831e-03 -8.0781982e-05\n",
      "  1.8292962e-03 -1.3691052e-03 -1.8714032e-03 -1.3240722e-03\n",
      " -1.4681795e-03 -2.6893933e-04 -4.8855512e-04  2.5973627e-03\n",
      "  1.1143468e-03 -4.1942624e-03 -1.3079669e-03  2.1445148e-03\n",
      "  1.5825141e-03 -2.1689801e-05  1.6255197e-03 -1.5522633e-03\n",
      "  1.4110407e-03 -3.2468964e-03 -3.6988861e-03 -1.8149740e-03\n",
      " -2.2246602e-03  2.2922899e-03 -5.8160292e-04 -4.3210201e-03\n",
      "  1.3877307e-03 -3.5223836e-04  2.6090306e-03 -4.1384413e-03\n",
      " -2.8422729e-03 -2.1840036e-03  2.0714928e-03  3.1742617e-03\n",
      " -2.0440873e-03  3.9476496e-03  4.8438385e-03  4.0205852e-03\n",
      "  3.8847954e-03 -2.3882107e-03 -1.7292149e-03 -4.1653854e-03\n",
      "  3.9300267e-03 -1.6816771e-03 -1.3306962e-03  5.4193131e-04\n",
      " -1.1171467e-03 -2.9313294e-04  4.0174956e-03 -3.2427539e-03\n",
      " -1.8198886e-03  4.3324120e-03 -1.1264439e-03  1.7907638e-03]\n"
     ]
    }
   ],
   "source": [
    "vocabulary2=embed.wv['way']\n",
    "print(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 84, 20)            1680      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 4,265\n",
      "Trainable params: 4,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                        input_shape=(84, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['categorical_accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 84, 128)           38400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84, 10)            1290      \n",
      "=================================================================\n",
      "Total params: 39,690\n",
      "Trainable params: 39,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), \n",
    "                               input_shape=(84,10)))\n",
    "#model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
